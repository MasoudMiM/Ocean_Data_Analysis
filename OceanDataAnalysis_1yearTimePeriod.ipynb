{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ocean Wave Data from Buoys Reported by National Data Buoy Center (NDBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [**Importing Necessary Libraries**](#0)\n",
    "\n",
    "2. [**Data Collection**](#10)\n",
    "\n",
    "3. [**A look at the Data**](#20)\n",
    "\n",
    "4. [**Exploratory Data Analysis**](#30)\n",
    "\n",
    "5. [**Model Development & Evaluation**](#40)\n",
    "  \n",
    "  5.1 [**K-Means Clustering**](#45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Necessary Libraries <a class=\"anchor\" id=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's install and import the required libraries\n",
    "# Uncomments the following lines in case you don't have the libraries installed on your machine\n",
    "#!conda install -c conda-forge beautifulsoup4 --yes \n",
    "#!conda install -c conda-forge lxml --yes\n",
    "#!conda install -c conda-forge requests --yes\n",
    "#!conda install -c conda-forge folium=0.9 --yes\n",
    "#!conda install -c conda-forge windrose=1.6.7 --yes\n",
    "#!conda install -c conda-forge ipywebrtc --yes\n",
    "\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen, Request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from windrose import WindroseAxes\n",
    "import folium\n",
    "import json\n",
    "from datetime import date, timedelta, datetime\n",
    "# Put figures in the center\n",
    "from IPython.core.display import HTML\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "print('All libraries installed and imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection <a class=\"anchor\" id=\"10\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the list of all the buoys from National Data Buoy Center. To get access to the data, we need to first create an account [here](https://data.planetos.com/datasets/noaa_ndbc_stdmet_stations?utm_source=github&utm_medium=notebook&utm_campaign=ndbc-wavewatch-iii-notebook). Once the account is created, API Key can be found under __Account Setting__. The data set id for National Data Buoy Center (NDBC) Standard Meteorological Data is __noaa_ndbc_stdmet_stations__. This data typically updates every hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we use an API to get Buys' latitude, longitude and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ hidden\n",
    "APIKey = \n",
    "DatasetID = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_url = 'http://api.planetos.com/v1/datasets/%s/stations?apikey=%s' % (DatasetID, APIKey)\n",
    "request = requests.get(API_url).json() #Request(API_url)\n",
    "Stations = pd.DataFrame(request['station'])\n",
    "df_stations = pd.DataFrame(Stations.columns)\n",
    "for indx, StName in enumerate(Stations.columns):\n",
    "    df_stations.loc[indx,1]=Stations.loc['SpatialExtent'][indx]['coordinates'][0]\n",
    "    df_stations.loc[indx,2]=Stations.loc['SpatialExtent'][indx]['coordinates'][1]\n",
    "\n",
    "df_stations.columns = ['Station_Name','Latitude','Longitude']\n",
    "print('number of stations:',df_stations.shape[0])\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the data from this API is not complete for each buoy and also have missing columns, we use the National Data Buoy Center website directly at https://www.ndbc.noaa.gov/ to get our information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a list of stations categorized by their owners [here](https://www.ndbc.noaa.gov/to_station.shtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=requests.get('https://www.ndbc.noaa.gov/to_station.shtml').text\n",
    "soup = BeautifulSoup(source,'lxml')\n",
    "Owners=[]\n",
    "BuoyData={}\n",
    "\n",
    "for owner in soup.find_all('h4'):\n",
    "    Owners.append(owner.text)\n",
    "for indx,table in enumerate(soup.find_all('pre')):\n",
    "    #print(indx,table.text)\n",
    "    try:\n",
    "        BuoyData[Owners[indx]]=table.text.replace('\\n','').strip()\n",
    "    except:\n",
    "        BuoyData[Owners[indx]]=table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's make all the station names uppercase since they are in lowercase format in the website\n",
    "df_stations['Station_Name']=df_stations['Station_Name'].str.upper()\n",
    "# Now, let's put this data and the data containing the latitudes and longitudes in the same dataframe\n",
    "newlist1=[]\n",
    "newlist2=[]\n",
    "for indx1 in range(df_stations.shape[0]):\n",
    "    for indx2,owner in enumerate(Owners):\n",
    "        if (df_stations.iloc[indx1,0] in BuoyData[owner]):\n",
    "            newlist1.append(owner)\n",
    "            newlist2.append(df_stations.iloc[indx1,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations0=pd.DataFrame({'Station_Name':newlist2,'Owner':newlist1})\n",
    "df_full1=pd.merge(df_stations0, df_stations, on='Station_Name')\n",
    "print('Number of Stations:',df_full1.shape[0])\n",
    "df_full1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuoyLocationPlot2(buoys,MapInput,DataFrameInput,colorm,fill_colorm,Marker=False):\n",
    "    \n",
    "    # loop through and add each to the feature group\n",
    "    for lat, lng, label in zip(DataFrameInput.Longitude, DataFrameInput.Latitude, DataFrameInput.Station_Name):\n",
    "        buoys.add_child(\n",
    "            folium.CircleMarker(\n",
    "                [lat, lng],\n",
    "                radius=6, # define how big you want the circle markers to be\n",
    "                color=colorm,\n",
    "                fill=True,\n",
    "                fill_color=fill_colorm,\n",
    "                fill_opacity=0.8,\n",
    "                popup=label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \n",
    "    if Marker==True:\n",
    "        latitudes = list(DataFrameInput.Longitude)\n",
    "        longitudes = list(DataFrameInput.Latitude)\n",
    "        labels = list(DataFrameInput.Station_Name)\n",
    "        for lat, lng, label in zip(latitudes, longitudes, labels):\n",
    "            folium.Marker([lat, lng], popup=label).add_to(MapInput)    \n",
    "    \n",
    "    \n",
    "    return buoys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Data for each buoy\n",
    "Not all the buys have data available for the same period of time. Some have been working for a short period of time, some had worked before but not any more, and some of them recently started working and collecting data. To have a full collection of the time range of the data available for each buoy, the following page is used from National Data Buoy Center website:\n",
    "https://www.ndbc.noaa.gov/historical_data.shtml#stdmet. It is worth mentioning that we are only interested in Standard Meterological data. To have a better understanding of  Standard Meterological data, look at the information provided here: https://www.ndbc.noaa.gov/measdes.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=requests.get('https://www.ndbc.noaa.gov/historical_data.shtml').text\n",
    "soup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "BuoyYearData={}\n",
    "BigTable=soup.find_all('ul')[1]\n",
    "SmallTable=BigTable.li.ul\n",
    "\n",
    "for indx,subtable in enumerate(SmallTable.find_all('li')):\n",
    "    StatName=SmallTable.find_all('li')[indx].text.split('\\n')[0].replace(':','')\n",
    "    est=SmallTable.find_all('li')[indx].text.split('\\n')\n",
    "    tmplist = est[1:]\n",
    "    for indx00,value in enumerate(tmplist):\n",
    "        tmplist[indx00] = tmplist[indx00].strip()\n",
    "    del tmplist[-1]\n",
    "    BuoyYearData[StatName]=tmplist\n",
    "    \n",
    "\n",
    "# Let's add one column for each year to the main dataframe\n",
    "YearsRange = range(1970,2019)\n",
    "df_full = df_full1.copy(deep=True)\n",
    "df_full.set_index(['Station_Name'],inplace=True)\n",
    "for indx, year in enumerate(YearsRange):\n",
    "    df_full[year] = None\n",
    "\n",
    "for indx,StName in enumerate(df_full.index):\n",
    "    first=np.array(df_full.loc[StName][4:].index).astype('int64')\n",
    "    try:\n",
    "        second=np.array(BuoyYearData[StName]).astype('int64')\n",
    "        df_full.loc[StName ,np.intersect1d(first,second)]=1\n",
    "    except:\n",
    "        df_full.loc[StName ,np.intersect1d(first,second)]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the buoys with their corresponding owners\n",
    "figmp=folium.Figure(width=1300, height=700)\n",
    "WorldMap_map2 = folium.Map(location=[17.6078, -8.0817],tiles=\"Stamen Toner\",zoom_start=2).add_to(figmp)\n",
    "color=iter(plt.cm.rainbow(np.linspace(0,1,len(Owners))))\n",
    "for indx,owner in enumerate(Owners):\n",
    "    clr=matplotlib.colors.to_hex(next(color))\n",
    "    feature_group = folium.map.FeatureGroup(name=owner)\n",
    "    df_tmp = df_full1[df_full1['Owner']==owner]\n",
    "    buoys=BuoyLocationPlot2(feature_group,WorldMap_map2,df_tmp,clr,clr)\n",
    "    WorldMap_map2.add_child(buoys,name=owner,index=indx)\n",
    "    \n",
    "folium.TileLayer(\"Stamen Toner\").add_to(WorldMap_map2) \n",
    "folium.TileLayer(\"OpenStreetMap\").add_to(WorldMap_map2) \n",
    "folium.TileLayer(\"Stamen Terrain\").add_to(WorldMap_map2)\n",
    "WorldMap_map2.add_child(folium.map.LayerControl())\n",
    "WorldMap_map2.save(\"WorldMapBuoysOwners.html\")\n",
    "WorldMap_map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data for a specific time period for all the buoys on the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "BYear=input('Enter the year in the format YYYY: ')\n",
    "\n",
    "MonthsNames=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "Months={MonthsNames[0]:str(BYear)+'-01-01 00:00:00',MonthsNames[1]:str(BYear)+'-02-01 00:00:00',MonthsNames[2]:str(BYear)+'-03-01 00:00:00',\n",
    "        MonthsNames[3]:str(BYear)+'-04-01 00:00:00',MonthsNames[4]:str(BYear)+'-05-01 00:00:00',MonthsNames[5]:str(BYear)+'-06-01 00:00:00',\n",
    "        MonthsNames[6]:str(BYear)+'-07-01 00:00:00',MonthsNames[7]:str(BYear)+'-08-01 00:00:00',MonthsNames[8]:str(BYear)+'-09-01 00:00:00',\n",
    "        MonthsNames[9]:str(BYear)+'-10-01 00:00:00',MonthsNames[10]:str(BYear)+'-11-01 00:00:00',MonthsNames[11]:str(BYear)+'-12-01 00:00:00'}\n",
    "\n",
    "\n",
    "startD=input('Enter the start date in the format DDMM like 10Jan: ')\n",
    "endD=input('Enter the end date in the format DDMM like 25Dec: ')\n",
    "date_start = startD+str(BYear)\n",
    "date_end = endD+str(BYear)\n",
    "date_st = datetime.strptime(date_start, \"%d%b%Y\")\n",
    "date_en = datetime.strptime(date_end, \"%d%b%Y\")\n",
    "print()\n",
    "print('start date:',date_st)\n",
    "print('end date:',date_en)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "# To record the data for all the buoys\n",
    "column_names_tmp = [\"Station_Name\",\"AVG_WDIR\", \"AVG_WSPD\", \"AVG_WVHT\", \"AVG_APD\", \"AVG_MWD\"]\n",
    "Finaldf = pd.DataFrame(columns = column_names_tmp)\n",
    "\n",
    "NoData=[] # To record the list of buoys with no data at the time period\n",
    "count=0\n",
    "for ii in range(len(df_full.index)):\n",
    "    BName=df_full.index[ii]\n",
    "    print('')\n",
    "    print('Name of the station:'+str(BName))\n",
    "    try: \n",
    "        print(BYear in BuoyYearData[BName])\n",
    "        if (BYear in BuoyYearData[BName]):\n",
    "            print('Available annual data for this buoy:',BuoyYearData[BName])\n",
    "            print('Data is available for buoy '+str(BName))\n",
    "            print(\"Now, let's continue!\")\n",
    "            url = \"https://www.ndbc.noaa.gov/view_text_file.php?filename={}h{}.txt.gz&dir=data/historical/stdmet/\".format(BName.lower(),BYear)\n",
    "            print(\"Downloading the data ...\")\n",
    "            print(url)\n",
    "            df_BName = pd.read_csv(url,na_values=[99,999,9999],delim_whitespace=True, skiprows=range(1,2))\n",
    "            print('Download is done!')\n",
    "\n",
    "            # Concatonates (#YY MM DD hh mm) into one column as DATE\n",
    "            df_BName.rename(columns={'#YY':'year','MM':'month','DD':'day','hh':'hour','mm':'minute'},inplace=True)\n",
    "            df_BName['DATE']=pd.to_datetime(df_BName[['year', 'month', 'day','hour','minute']])\n",
    "            df_BName.drop(columns=['year','month','day','hour','minute'],inplace=True)\n",
    "        \n",
    "            df_BName=df_BName.loc[(df_BName['DATE']>=date_st) & (df_BName['DATE']<=date_en)]\n",
    "            df_BName.sort_values(by='DATE',ascending=False,inplace=True)\n",
    "            df_BName.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "            listLabels = ['Wind Dir. (deg)', 'Wind speed (m/s)', 'Sign. Wave Height (m)', 'Domi. Wave Period (s)',\\\n",
    "                          'Ave. Wave Period (s)','Wave Dir. (deg)', 'Sea Level Pressure (hPa)','Air Temp. (C)', 'Sea Surface Temp. (C)',\\\n",
    "                          'Dewpoint Temp.','Station Visib. (nautical miles)','Water Level']\n",
    "            df_BName.drop(columns=['GST'],inplace=True)\n",
    "            \n",
    "            df_BName=df_BName.dropna(axis=1, how=\"all\")\n",
    "            \n",
    "            # Looking at each month\n",
    "            hours=np.linspace(0,23,num=24)\n",
    "            hrdic ={}\n",
    "            emptydaysinamonth={}\n",
    "            for idd,monthh in enumerate(MonthsNames):\n",
    "                try:\n",
    "                    mask = (df_BName['DATE'] >= Months[MonthsNames[idd]]) & (df_BName['DATE'] < Months[MonthsNames[idd+1]])\n",
    "                except:\n",
    "                    mask = df_BName['DATE'] >= Months[MonthsNames[idd]]\n",
    "\n",
    "                df_month=df_BName.loc[mask]\n",
    "\n",
    "                try:\n",
    "                    days = pd.date_range(Months[MonthsNames[idd]],Months[MonthsNames[idd+1]],freq='d',closed='left')\n",
    "                except:\n",
    "                    days = pd.date_range(str(BYear)+'-12-01',str(BYear)+'-12-31',freq='d',closed='left')\n",
    "\n",
    "                # looking at each day within the month\n",
    "                emptydays=[]\n",
    "                for did,dayy in enumerate(days):\n",
    "\n",
    "\n",
    "                    if (dayy.day not in pd.DatetimeIndex(df_month['DATE']).day):\n",
    "                        emptydays.append(dayy.day)\n",
    "\n",
    "                    else:\n",
    "                        df_day=df_month[pd.DatetimeIndex(df_month['DATE']).day==dayy.day]\n",
    "\n",
    "                        # looking at hours in a day\n",
    "                        boolholder=0\n",
    "                        for idh,hr in enumerate(hours):\n",
    "                            if (hr in pd.DatetimeIndex(df_day['DATE']).hour):\n",
    "                                boolholder+=1\n",
    "\n",
    "                        if (24-boolholder)>=22:\n",
    "                            emptydays.append(dayy.day)\n",
    "            \n",
    "                emptydaysinamonth[monthh]=emptydays\n",
    "            \n",
    "                # counting the number of consequetive days\n",
    "                consdays=np.count_nonzero(np.diff(np.array(emptydaysinamonth[monthh]))==1)\n",
    "            \n",
    "                #print('Empty days in ',monthh,' are:',emptydaysinamonth[monthh])\n",
    "                #print('Number of empty days in ',monthh,' is:',len(emptydaysinamonth[monthh]))\n",
    "                #print('Number of consequetive days with no data:',consdays)\n",
    "\n",
    "                if (len(emptydaysinamonth[monthh])>=11 or consdays>=5):\n",
    "                    #print(idd)\n",
    "                    try:\n",
    "                        df_BName[pd.DatetimeIndex(df_BName[\"DATE\"]).month==idd+1]=None\n",
    "                    except:\n",
    "                        None\n",
    "                        \n",
    "                \n",
    "                df_BName=df_BName.dropna(axis=0, how=\"all\")\n",
    "    \n",
    "        \n",
    "            if len(pd.DatetimeIndex(df_BName[\"DATE\"]).month.value_counts())>=8:\n",
    "                Finaldf = Finaldf.append({'Station_Name': BName, 'AVG_WDIR': df_BName[\"WDIR\"].mean() if 'WDIR' in df_BName.columns else None,\n",
    "                                          'AVG_WSPD': df_BName[\"WSPD\"].mean() if 'WSPD' in df_BName.columns else None,\n",
    "                                          'AVG_WVHT':df_BName[\"WVHT\"].mean() if 'WVHT' in df_BName.columns else None,\n",
    "                                          'AVG_APD':df_BName[\"APD\"].mean() if 'APD' in df_BName.columns else None,\n",
    "                                          'AVG_MWD':df_BName[\"MWD\"].mean() if 'MWD' in df_BName.columns else None}, ignore_index=True)\n",
    "                    \n",
    "                    \n",
    "                count=count+1\n",
    "                print(count)\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                NoData.append(BName)\n",
    "            \n",
    "        else:\n",
    "            print('Buoy '+str(BName)+' has no data recorded for this period.')\n",
    "    except:\n",
    "        print('Buoy '+str(BName)+' has no data recorded for this period.')\n",
    "        NoData.append(BName) # Stores the name of the buoys with no recorded data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.DatetimeIndex(df_BName[\"DATE\"]).month.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of stations with recorded data from '+str(date_st)+' to '+ str(date_en)+ ' is '+ str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finaldf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis <a class=\"anchor\" id=\"30\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's take a look at the general where the information for all five parameters AVG_WDIR, AVG_WSPD, AVG_WVHT, and AVG_ADP, AVG_MWD.\n",
    "\n",
    "__WDIR__: Wind direction (the direction the wind is coming from in degrees clockwise from true N) during the same period used for WSPD\n",
    "\n",
    "__WSPD__: Wind speed (m/s) averaged over an eight-minute period for buoys and a two-minute period for land stations. Reported Hourly. \n",
    "\n",
    "__WVHT__: Significant wave height (meters) is calculated as the average of the highest one-third of all of the wave heights during the 20-minute sampling period.\n",
    "\n",
    "__APD__: Average wave period (seconds) of all waves during the 20-minute period.\n",
    "\n",
    "__MWD__: The direction from which the waves at the dominant period (DPD) are coming. The units are degrees from true North, increasing clockwise, with North as 0 (zero) degrees and East as 90 degrees. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that we are Not interested in\n",
    "df_analysis=Finaldf.drop(columns=['AVG_MWD','AVG_WDIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataframe without Nan values\n",
    "df_analysis=df_analysis.dropna(axis=0)\n",
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of remaining stations without any nan values for the four features mentioned above: '+str(len(df_analysis.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's find out if there is any correlation between these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, axes = plt.subplots(1,3,figsize=(20, 8))\n",
    "#fig.set_size_inches(25, 10)\n",
    "#sns.scatterplot(x=\"AVG_WDIR\", y=\"AVG_WSPD\",data=df_analysis, s=200 ,ax=axes[0,0])\n",
    "#axes[0,0].set_xlabel('Avegare Wind Dir. (Deg)')\n",
    "#axes[0,0].set_ylabel('Average Wind Speed (m/s)')\n",
    "\n",
    "#sns.scatterplot(x=\"AVG_WDIR\", y=\"AVG_WVHT\",data=df_analysis, s=200 ,ax=axes[0,1])\n",
    "#axes[0,1].set_xlabel('Avegare Wind Dir. (Deg)')\n",
    "#axes[0,1].set_ylabel('Avegare Sig. Wave Height (m)')\n",
    "\n",
    "#sns.scatterplot(x=\"AVG_WDIR\", y=\"AVG_APD\",data=df_analysis, s=200 ,ax=axes[1,0])\n",
    "#axes[1,0].set_xlabel('Avegare Wind Dir. (Deg)')\n",
    "#axes[1,0].set_ylabel('Average Wave Period (s)')\n",
    "\n",
    "sns.scatterplot(x=\"AVG_WSPD\", y=\"AVG_WVHT\",data=df_analysis, s=200 ,ax=axes[0])\n",
    "axes[0].set_xlabel('Ave Wind Speed (m/s)')\n",
    "axes[0].set_ylabel('Ave Wave Height (m)')\n",
    "\n",
    "sns.scatterplot(x=\"AVG_WSPD\", y=\"AVG_APD\",data=df_analysis, s=200 ,ax=axes[1])\n",
    "axes[1].set_xlabel('Ave Wind Speed (m/s)')\n",
    "axes[1].set_ylabel('Ave Wave Period (s)')\n",
    "\n",
    "sns.scatterplot(x=\"AVG_WVHT\", y=\"AVG_APD\",data=df_analysis, s=200 ,ax=axes[2])\n",
    "axes[2].set_xlabel('Ave Wave Height (m)')\n",
    "axes[2].set_ylabel('Ave Wave Period (s)')\n",
    "\n",
    "plt.savefig('Exploratory.eps',bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis_mod=df_analysis.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, axes = plt.subplots(1,3,figsize=(20, 8))\n",
    "\n",
    "sns.scatterplot(x=\"AVG_WSPD\", y=\"AVG_WVHT\",data=df_analysis_mod, s=200 ,ax=axes[0])\n",
    "axes[0].set_xlabel('Ave Wind Speed (m/s)')\n",
    "axes[0].set_ylabel('Ave Wave Height (m)')\n",
    "\n",
    "sns.scatterplot(x=\"AVG_WSPD\", y=\"AVG_APD\",data=df_analysis_mod, s=200 ,ax=axes[1])\n",
    "axes[1].set_xlabel('Ave Wind Speed (m/s)')\n",
    "axes[1].set_ylabel('Ave Wave Period (s)')\n",
    "\n",
    "sns.scatterplot(x=\"AVG_WVHT\", y=\"AVG_APD\",data=df_analysis_mod, s=200 ,ax=axes[2])\n",
    "axes[2].set_xlabel('Ave Wave Height (m)')\n",
    "axes[2].set_ylabel('Ave Wave Period (s)')\n",
    "\n",
    "plt.savefig('Exploratory.eps',bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the stations with the recorded data\n",
    "df_analysis_modplot=pd.merge(df_analysis_mod, df_full1, on='Station_Name')\n",
    "\n",
    "feature_group = folium.map.FeatureGroup()\n",
    "\n",
    "figmp=folium.Figure(width=1300, height=700)\n",
    "WorldMap_map3 = folium.Map(location=[17.6078, -8.0817],tiles=\"Stamen Toner\",zoom_start=2).add_to(figmp)\n",
    "color=iter(plt.cm.rainbow(np.linspace(0,1,len(df_analysis_modplot.Station_Name))))\n",
    "\n",
    "buoys=BuoyLocationPlot2(feature_group,WorldMap_map3,df_analysis_modplot,'red','blue')\n",
    "WorldMap_map3.add_child(buoys,name=owner,index=indx)\n",
    "    \n",
    "folium.TileLayer(\"Stamen Toner\").add_to(WorldMap_map3) \n",
    "folium.TileLayer(\"OpenStreetMap\").add_to(WorldMap_map3) \n",
    "folium.TileLayer(\"Stamen Terrain\").add_to(WorldMap_map3)\n",
    "WorldMap_map3.add_child(folium.map.LayerControl())\n",
    "WorldMap_map3.save(\"WorldMapBuoysAnalysis.html\")\n",
    "WorldMap_map3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Development and Evaluation <a class=\"anchor\" id=\"40\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 K-Means Clustering <a class=\"anchor\" id=\"45\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have variables with different ranges, let's normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feaures=StandardScaler().fit_transform(df_analysis_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to determine the right value for __k__. To do so, we have two possible approaches:\n",
    "\n",
    "- **Elbow Method**\n",
    "- **Silhouette analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Elbow method__ gives us an idea on what a good k number of clusters would be based on the sum of squared distance (SSE) between data points and their assigned clusters’ centroids. We pick k at the spot where SSE starts to flatten out and forming an elbow. We evaluate SSE for different values of k and see where the curve might form an elbow and flatten out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to find the best k value\n",
    "sse = []\n",
    "list_k = list(range(1, 20))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(init = \"k-means++\",n_clusters=k)\n",
    "    km.fit(Feaures)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(list_k, sse, '-o', markersize=10, linewidth=4)\n",
    "plt.xlabel(r'Number of clusters [k]')\n",
    "plt.ylabel('SSE');\n",
    "\n",
    "plt.savefig('ElbowMethod.eps',bbox_inches='tight',dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not see a clear elbow shape using this method to determine the value of k, let's try Silhouette analysis. **Silhouette analysis** can be used to determine the degree of separation between clusters. For each sample:\n",
    "- Compute the average distance from all data points in the same cluster (ai).\n",
    "- Compute the average distance from all data points in the closest cluster (bi).\n",
    "- Compute the coefficient.\n",
    "\n",
    "The coefficient can take values in the interval [-1, 1].\n",
    "- If  0 : the sample is very close to the neighboring clusters.\n",
    "- If  1 : the sample is far away from the neighboring clusters.\n",
    "- If -1 : the sample is assigned to the wrong clusters.\n",
    "\n",
    "Therefore, we want the coefficients to be as big as possible and close to 1 to have good clusters. See the full explanation here: https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(20, 15))\n",
    "axs = axs.ravel()\n",
    "#fig.subplots_adjust(hspace = .5, wspace=.2)\n",
    "\n",
    "for i, k in enumerate([2, 3, 4, 5, 6, 7]):\n",
    "    # Run the Kmeans algorithm\n",
    "    km = KMeans(init = \"k-means++\",n_clusters=k)\n",
    "    labels = km.fit_predict(Feaures)\n",
    "    centroids = km.cluster_centers_\n",
    "\n",
    "    # Get silhouette samples\n",
    "    silhouette_vals = silhouette_samples(Feaures, labels,metric='euclidean')\n",
    "\n",
    "    # Silhouette plot\n",
    "    y_ticks = []\n",
    "    y_lower, y_upper = 0, 0\n",
    "    for ii, cluster in enumerate(np.unique(labels)):\n",
    "        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "        cluster_silhouette_vals.sort()\n",
    "        y_upper += len(cluster_silhouette_vals)\n",
    "        axs[i].barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "        axs[i].text(-0.05, (y_lower + y_upper) / 2, str(ii + 1))\n",
    "        y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "    # Get the average silhouette score and plot it\n",
    "    avg_score = np.mean(silhouette_vals)\n",
    "    axs[i].axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "    axs[i].set_yticks([])\n",
    "    axs[i].set_xlim([-0.1, 1])\n",
    "    axs[i].set_xlabel('Silhouette coefficient values')\n",
    "    axs[i].set_ylabel('Cluster labels')\n",
    "\n",
    "plt.savefig('Silhouette2.eps',bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's choose k=5\n",
    "kclusters=5\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(init = \"k-means++\",n_clusters=kclusters, random_state=0).fit(Feaures)\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "print(kmeans.labels_[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clustering labels to the data frame\n",
    "try:\n",
    "    df_analysis_mod.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "except:\n",
    "    df_analysis_mod['Cluster Labels']=kmeans.labels_\n",
    "    \n",
    "df_analysis_mod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map\n",
    "df_analysis_modKMeansplot=pd.merge(df_analysis_mod, df_full1, on='Station_Name')\n",
    "\n",
    "map_clusters = folium.Map(location=[17.6078, -8.0817], tiles=\"Stamen Toner\", zoom_start=2)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(df_analysis_modKMeansplot['Longitude'], df_analysis_modKMeansplot['Latitude'], df_analysis_modKMeansplot['Station_Name'], df_analysis_modKMeansplot['Cluster Labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters.save(\"WorldMapBuoysAnalysis_Clustering.html\")\n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis_modKMeansplot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clust_I = df_analysis_mod[df_analysis_mod['Cluster Labels']==0]\n",
    "print('Stations in Cluster I:')\n",
    "print(df_clust_I.Station_Name)\n",
    "print()\n",
    "df_clust_II = df_analysis_mod[df_analysis_mod['Cluster Labels']==1]\n",
    "print('Stations in Cluster II:')\n",
    "print(df_clust_II.Station_Name)\n",
    "print()\n",
    "df_clust_III = df_analysis_mod[df_analysis_mod['Cluster Labels']==2]\n",
    "print('Stations in Cluster III:')\n",
    "print(df_clust_III.Station_Name)\n",
    "print()\n",
    "df_clust_IV = df_analysis_mod[df_analysis_mod['Cluster Labels']==3]\n",
    "print('Stations in Cluster IV:')\n",
    "print(df_clust_IV.Station_Name)\n",
    "df_clust_V = df_analysis_mod[df_analysis_mod['Cluster Labels']==4]\n",
    "print('Stations in Cluster V:')\n",
    "print(df_clust_V.Station_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(25, 12))\n",
    "sns.boxplot(x=\"Cluster Labels\", y=\"AVG_WSPD\", data=df_analysis_mod,ax=axs[0])\n",
    "sns.swarmplot(x=\"Cluster Labels\", y=\"AVG_WSPD\", data=df_analysis_mod,color=\".15\",ax=axs[0])\n",
    "axs[0].set_xlabel('Clusters');\n",
    "axs[0].set_ylabel('Ave Wind Speed (m/s)');\n",
    "axs[0].set_xticklabels([1,2,3,4,5])\n",
    "#axs[0].set_xticks([0,1,2,3,4],[1,2,3,4,5]);\n",
    "\n",
    "sns.boxplot(x=\"Cluster Labels\", y=\"AVG_WVHT\", data=df_analysis_mod,ax=axs[1])\n",
    "sns.swarmplot(x=\"Cluster Labels\", y=\"AVG_WVHT\", data=df_analysis_mod,color=\".15\",ax=axs[1])\n",
    "axs[1].set_xlabel('Clusters');\n",
    "axs[1].set_ylabel('Ave Wave Height (m)');\n",
    "axs[1].set_xticklabels([1,2,3,4,5])\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Cluster Labels\", y=\"AVG_APD\", data=df_analysis_mod,ax=axs[2])\n",
    "sns.swarmplot(x=\"Cluster Labels\", y=\"AVG_APD\", data=df_analysis_mod,color=\".15\",ax=axs[2])\n",
    "axs[2].set_xlabel('Clusters');\n",
    "axs[2].set_ylabel('Ave Wave Period (s)');\n",
    "axs[2].set_xticklabels([1,2,3,4,5])\n",
    "\n",
    "plt.savefig('BoxPlot1Year.eps',bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax1=plt.subplots(1,1,figsize=(25,10))\n",
    "labels = kmeans.labels_\n",
    "area1 = np.pi * ( df_analysis_mod['AVG_WSPD'] )**(2.5)\n",
    "scatter=ax1.scatter(df_analysis_mod['AVG_WVHT'], df_analysis_mod['AVG_APD'],s=area1.astype(np.float), label=labels, c=labels.astype(np.float),edgecolors='blue', alpha=0.9)\n",
    "ax1.set_ylabel('Ave Wave Height (m)', fontsize=35)\n",
    "ax1.set_xlabel('Ave Wave Period (s)', fontsize=35)\n",
    "\n",
    "# produce a legend with the unique colors from the scatter\n",
    "legend1 = ax1.legend(*scatter.legend_elements(),loc=\"upper left\",prop={'size': 30},markerscale=4 ,title=\"Clusters\")\n",
    "# fixing the numbering in legend\n",
    "legend1.texts[0].set_text('$\\\\mathdefault{I}$')\n",
    "legend1.texts[1].set_text('$\\\\mathdefault{II}$')\n",
    "legend1.texts[2].set_text('$\\\\mathdefault{III}$')\n",
    "legend1.texts[3].set_text('$\\\\mathdefault{IV}$')\n",
    "legend1.texts[4].set_text('$\\\\mathdefault{V}$')\n",
    "\n",
    "ax1.add_artist(legend1)\n",
    "# produce a legend with a cross section of sizes from the scatter\n",
    "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.6)\n",
    "legend2 = ax1.legend(handles, labels, loc=\"lower right\", title=\"$(WSPD)^{2.5}$\")\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig('ClusterPlots.eps',bbox_inches='tight',dpi=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Mean values for cluster I:')\n",
    "print(df_clust_I[['AVG_WSPD','AVG_WVHT','AVG_APD']].mean())\n",
    "print('')\n",
    "print('Mean values for cluster II:')\n",
    "print(df_clust_II[['AVG_WSPD','AVG_WVHT','AVG_APD']].mean())\n",
    "print('')\n",
    "print('Mean values for cluster III:')\n",
    "print(df_clust_III[['AVG_WSPD','AVG_WVHT','AVG_APD']].mean())\n",
    "print('')\n",
    "print('Mean values for cluster IV:')\n",
    "print(df_clust_IV[['AVG_WSPD','AVG_WVHT','AVG_APD']].mean())\n",
    "print('Mean values for cluster V:')\n",
    "print(df_clust_V[['AVG_WSPD','AVG_WVHT','AVG_APD']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust_II=df_clust_II.astype('float64')\n",
    "df_clust_II[df_clust_II['AVG_APD']<4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust_V=df_clust_V.astype('float64')\n",
    "df_clust_V.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust_I.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
